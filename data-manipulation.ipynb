{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The first data set we are going to analyze is the Enron Corpus.<br/>\n",
    "Because the corpus is so large, we really would not want to load all of the emails into python at once.<br/>\n",
    "Therefore, I have split the original csv, containing all 500k emails, into smaller files of about 10k emails each.<br/>\n",
    "We will start with loading the first snippet of the emails using pandas.<br/>\n",
    "<br/>\n",
    "Every cvs file has a header, telling pandas to record each email as two parts: a filename and the email's body.<br/>\n",
    "I also made sure that no emails are split between two different files (every email is intact within a single file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import email.utils\n",
    "import email\n",
    "import email.header\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10430, 2)\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(\"enron_dataset_unprocessed/enron emails_chunk1.csv\")\n",
    "print(file.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having read the csv in, we can see the \"shape\" of the file.<br/>\n",
    "Here, we have 7942 emails, each with 2 fields: a filename and a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allen-p/_sent_mail/1.\n"
     ]
    }
   ],
   "source": [
    "print(file['file'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <32259460.1075852688586.JavaMail.evans@thyme>\r\n",
      "Date: Fri, 5 Oct 2001 01:31:17 -0700 (PDT)\r\n",
      "From: jennifer.fraser@enron.com\r\n",
      "To: john.arnold@enron.com\r\n",
      "Subject: RE: right about now dont u think u otta sell some calls against yr\r\n",
      " 36.88s\r\n",
      "Mime-Version: 1.0\r\n",
      "Content-Type: text/plain; charset=us-ascii\r\n",
      "Content-Transfer-Encoding: 7bit\r\n",
      "X-From: Fraser, Jennifer </O=ENRON/OU=NA/CN=RECIPIENTS/CN=JFRASER>\r\n",
      "X-To: Arnold, John </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Jarnold>\r\n",
      "X-cc: \r\n",
      "X-bcc: \r\n",
      "X-Folder: \\JARNOLD (Non-Privileged)\\Arnold, John\\Deleted Items\r\n",
      "X-Origin: Arnold-J\r\n",
      "X-FileName: JARNOLD (Non-Privileged).pst\r\n",
      "\r\n",
      "becuase we are overvalued .... jan01 37.50 2.90 bid\r\n",
      "\r\n",
      " -----Original Message-----\r\n",
      "From: \tArnold, John  \r\n",
      "Sent:\tThursday, October 04, 2001 10:25 PM\r\n",
      "To:\tFraser, Jennifer\r\n",
      "Subject:\tRE: right about now dont u think u otta sell some calls against yr 36.88s\r\n",
      "\r\n",
      "because we're $10 off the lows or because you think we're overvalued?\r\n",
      "\r\n",
      " -----Original Message-----\r\n",
      "From: \tFraser, Jennifer  \r\n",
      "Sent:\tThursday, October 04, 2001 11:23 AM\r\n",
      "To:\tArnold, John\r\n",
      "Subject:\tright about now dont u think u otta sell some calls against yr 36.88s\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(file['message'][5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking Down our Features\n",
    "\n",
    "Not all of the information from the emails will be helpful. This experiment focuses mainly on the content of the body, and we hypothesize that the senders, receivers, dates, etc. will not be useful in classifying their topics. Our first step is to remove this information from the body while retaining it in our dataframe in case they <i>do</i> end up being important for classification later on in the experiment.<br/>\n",
    "\n",
    "We will need to construct a new header for each .csv containing our complete set of features.<br/>\n",
    "When we are done, our features will be:\n",
    "<ul>\n",
    "    <li>filename</li>\n",
    "    <li>id</li>\n",
    "    <li>date</li>\n",
    "    <li>from</li>\n",
    "    <li>to</li>\n",
    "    <li>subject</li>\n",
    "    <li>cc</li>\n",
    "    <li>mime-version</li>\n",
    "    <li>content-type</li>\n",
    "    <li>content-encoding</li>\n",
    "    <li>bcc</li>\n",
    "    <li>message</li>\n",
    "</ul>\n",
    "Although there are fields in each email such as \"X-From\" \"X-To\" etc., these are redundant and it should be safe to disregard them without losing any data.<br/><br/>\n",
    "Let's make a function that will construct a header and add it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_header(filepath):\n",
    "    # The header containing our complete set of features\n",
    "    header = \"\\\"filename\\\",\\\"id\\\",\\\"date\\\",\\\"from\\\",\\\"to\\\",\\\"subject\\\",\\\"cc\\\",\\\"mime-version\\\",\\\"content-type\\\",\\\"content-encoding\\\",\\\"bcc\\\",\\\"message\\\"\\n\"\n",
    "\n",
    "    # Create a new file in the \"processed\" folder \n",
    "    # The filename should correspond to the given filepath.\n",
    "    f = open(os.path.join(\"enron_dataset_processed\", os.path.basename(filepath)), \"w+\")\n",
    "    # Write the header to the file and print the first line of the file to confirm that the header is formatted correctly.\n",
    "    f.write(header)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write another function to take the lines from an email header and stick them into each of these features. This will be done by surrounding each feature with quotations inside the .csv.<br/>\n",
    "Our function will perform this action on all emails in a single chunk.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_features(filepath):\n",
    "    \n",
    "    # Read the file and collect all unprocessed emails\n",
    "    file = pd.read_csv(filepath)\n",
    "    \n",
    "    # Open the file in the \"processed\" folder.\n",
    "    # The filename should correspond to the given filepath.\n",
    "    f = open(os.path.join(\"enron_dataset_processed\", os.path.basename(filepath)), \"a+\")\n",
    "    \n",
    "    # For each email in the chunk, separate the features and write to a new file.\n",
    "    for x in range(0, file.shape[0]):\n",
    "    \n",
    "        lines = file['message'][x].splitlines()\n",
    "    \n",
    "        # First comes the filename\n",
    "        f.write(\"\\\"\" + file['file'][0] + \"\\\",\")\n",
    "\n",
    "        # Every email has a message id\n",
    "        f.write(\"\\\"\" + lines[0] + \"\\\",\")\n",
    "\n",
    "        # Every email has a date\n",
    "        f.write(\"\\\"\" + lines[1] + \"\\\",\")\n",
    "\n",
    "        # Every email has a from line\n",
    "        f.write(\"\\\"\" + lines[2] + \"\\\",\")\n",
    "\n",
    "        # Every email has a to line\n",
    "        f.write(\"\\\"\" + lines[3] + \"\\\",\")\n",
    "\n",
    "        # Every email has a subject line, even though some are left blank\n",
    "        f.write(\"\\\"\" + lines[4] + \"\\\",\")\n",
    "\n",
    "        # Not every email has a cc line.\n",
    "        cc_line = 0\n",
    "        if lines[5].startswith('Cc'):\n",
    "            f.write(\"\\\"\" + lines[5] + \"\\\",\")\n",
    "            cc_line = 1\n",
    "        else:\n",
    "            f.write(\"\\\" \\\",\")\n",
    "\n",
    "        # Every email has a mime version\n",
    "        f.write(\"\\\"\" + lines[5 + cc_line] + \"\\\",\")\n",
    "\n",
    "        # Every email has a content-type\n",
    "        f.write(\"\\\"\" + lines[6 + cc_line] + \"\\\",\")\n",
    "\n",
    "        # Every email has a content-encoding\n",
    "        f.write(\"\\\"\" + lines[7 + cc_line] + \"\\\",\")\n",
    "\n",
    "        # Not email has a bcc line\n",
    "        bcc_line = 0\n",
    "        if lines[8 + cc_line].startswith('Bcc'):\n",
    "            f.write(\"\\\"\" + lines[8 + cc_line] + \"\\\",\")\n",
    "            bcc_line = 1\n",
    "        else:\n",
    "            f.write(\"\\\" \\\",\")\n",
    "\n",
    "        # Whatever remains is part of the message\n",
    "        f.write(\"\\\"\")\n",
    "        for i in range(9 + cc_line + bcc_line, len(lines)):\n",
    "            # We don't need to hold on to fields that start with X- because these are duplicated from the file header.\n",
    "            # These lines are only found in the current email, so it will not alter the content of email chains.\n",
    "            if lines[i].startswith(\"X-\"):\n",
    "                continue\n",
    "            f.write(lines[i].replace(\"\\\"\", \"\\\"\\\"\") + \"\\n\")\n",
    "        f.write(\"\\\"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test these methods on our first chunk and make sure the formatting is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_header(\"enron_dataset_unprocessed/enron emails_chunk1.csv\")\n",
    "separate_features(\"enron_dataset_unprocessed/enron emails_chunk1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the file visually looks good... Let's see if we can read it in using pandas without any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10427, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8868: expected 12 fields, saw 13\\nSkipping line 9197: expected 12 fields, saw 13\\nSkipping line 9380: expected 12 fields, saw 13\\n'\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(\"enron_dataset_processed/enron emails_chunk1.csv\", error_bad_lines=False)\n",
    "print(file.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we've lost a couple of emails due to parsing errors. I need to look into this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
