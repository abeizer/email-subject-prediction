{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "Here, we pre-process data in the email bodies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the manipulated, unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Subject</th>\n",
       "      <th>X-From</th>\n",
       "      <th>X-To</th>\n",
       "      <th>X-cc</th>\n",
       "      <th>X-bcc</th>\n",
       "      <th>X-Folder</th>\n",
       "      <th>X-Origin</th>\n",
       "      <th>X-FileName</th>\n",
       "      <th>content</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;30054600.1075841565738.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2002-01-31 03:37:11</td>\n",
       "      <td>frozenset({'pete.davis@enron.com'})</td>\n",
       "      <td>frozenset({'pete.davis@enron.com'})</td>\n",
       "      <td>Start Date: 1/30/02; HourAhead hour: 22;</td>\n",
       "      <td>Schedule Crawler&lt;pete.davis@enron.com&gt;@ENRON</td>\n",
       "      <td>Davis, Pete &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=P...</td>\n",
       "      <td>Meyers, Albert &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\ExMerge - Solberg, Geir\\Deleted Items</td>\n",
       "      <td>SOLBERG-G</td>\n",
       "      <td>geir solberg 6-26-02.PST</td>\n",
       "      <td>\\r\\r\\n\\r\\r\\nStart Date: 1/30/02; HourAhead hou...</td>\n",
       "      <td>solberg-g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;33136589.1075852829115.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2001-10-12 03:36:25</td>\n",
       "      <td>frozenset({'402075.57130981.1@1.americanexpres...</td>\n",
       "      <td>frozenset({'kpresto@enron.com'})</td>\n",
       "      <td>Help the Sept. 11 disaster relief effort</td>\n",
       "      <td>Membership Rewards &lt;membershiprewards+402075.5...</td>\n",
       "      <td>kpresto@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\KPRESTO (Non-Privileged)\\Deleted Items</td>\n",
       "      <td>Presto-K</td>\n",
       "      <td>KPRESTO (Non-Privileged).pst</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>presto-k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;4916370.1075861400181.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2001-11-05 14:37:15</td>\n",
       "      <td>frozenset({'harry.arora@enron.com'})</td>\n",
       "      <td>frozenset({'iris.mack@enron.com'})</td>\n",
       "      <td>FW: Synthetic Peaker</td>\n",
       "      <td>Arora, Harry &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=...</td>\n",
       "      <td>Mack, Iris &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=Im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\HARORA (Non-Privileged)\\Arora, Harry\\Sent Items</td>\n",
       "      <td>Arora-H</td>\n",
       "      <td>HARORA (Non-Privileged).pst</td>\n",
       "      <td>\\r\\r\\n-----Original Message-----\\r\\r\\nFrom: B...</td>\n",
       "      <td>arora-h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;24276539.1075856274091.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2000-10-19 20:25:00</td>\n",
       "      <td>frozenset({'enron.announcements@enron.com'})</td>\n",
       "      <td>frozenset({'all_ena_egm_eim@enron.com'})</td>\n",
       "      <td>Ameriflash Newsletter</td>\n",
       "      <td>Enron Announcements</td>\n",
       "      <td>All_ENA_EGM_EIM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\Vincent_Kaminski_Jun2001_1\\Notes Folders\\All ...</td>\n",
       "      <td>Kaminski-V</td>\n",
       "      <td>vkamins.nsf</td>\n",
       "      <td>NOTE FROM MARK FREVERT\\r\\r\\n\\r\\r\\nWith the wid...</td>\n",
       "      <td>kaminski-v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;28413439.1075854360990.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2001-04-05 13:17:00</td>\n",
       "      <td>frozenset({'darron.giron@enron.com'})</td>\n",
       "      <td>frozenset({'phillip.love@enron.com'})</td>\n",
       "      <td>FW: the mullet!:</td>\n",
       "      <td>Darron C Giron</td>\n",
       "      <td>Phillip M Love</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\Darron_Giron_Jun2001\\Notes Folders\\All documents</td>\n",
       "      <td>Giron-D</td>\n",
       "      <td>dgiron.nsf</td>\n",
       "      <td>---------------------- Forwarded by Darron C G...</td>\n",
       "      <td>giron-d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Message-ID                 Date  \\\n",
       "0  <30054600.1075841565738.JavaMail.evans@thyme>  2002-01-31 03:37:11   \n",
       "1  <33136589.1075852829115.JavaMail.evans@thyme>  2001-10-12 03:36:25   \n",
       "2   <4916370.1075861400181.JavaMail.evans@thyme>  2001-11-05 14:37:15   \n",
       "3  <24276539.1075856274091.JavaMail.evans@thyme>  2000-10-19 20:25:00   \n",
       "4  <28413439.1075854360990.JavaMail.evans@thyme>  2001-04-05 13:17:00   \n",
       "\n",
       "                                                From  \\\n",
       "0                frozenset({'pete.davis@enron.com'})   \n",
       "1  frozenset({'402075.57130981.1@1.americanexpres...   \n",
       "2               frozenset({'harry.arora@enron.com'})   \n",
       "3       frozenset({'enron.announcements@enron.com'})   \n",
       "4              frozenset({'darron.giron@enron.com'})   \n",
       "\n",
       "                                         To  \\\n",
       "0       frozenset({'pete.davis@enron.com'})   \n",
       "1          frozenset({'kpresto@enron.com'})   \n",
       "2        frozenset({'iris.mack@enron.com'})   \n",
       "3  frozenset({'all_ena_egm_eim@enron.com'})   \n",
       "4     frozenset({'phillip.love@enron.com'})   \n",
       "\n",
       "                                    Subject  \\\n",
       "0  Start Date: 1/30/02; HourAhead hour: 22;   \n",
       "1  Help the Sept. 11 disaster relief effort   \n",
       "2                      FW: Synthetic Peaker   \n",
       "3                     Ameriflash Newsletter   \n",
       "4                          FW: the mullet!:   \n",
       "\n",
       "                                              X-From  \\\n",
       "0       Schedule Crawler<pete.davis@enron.com>@ENRON   \n",
       "1  Membership Rewards <membershiprewards+402075.5...   \n",
       "2  Arora, Harry </O=ENRON/OU=NA/CN=RECIPIENTS/CN=...   \n",
       "3                                Enron Announcements   \n",
       "4                                     Darron C Giron   \n",
       "\n",
       "                                                X-To  \\\n",
       "0  Davis, Pete </O=ENRON/OU=NA/CN=RECIPIENTS/CN=P...   \n",
       "1                                  kpresto@enron.com   \n",
       "2  Mack, Iris </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Im...   \n",
       "3                                    All_ENA_EGM_EIM   \n",
       "4                                     Phillip M Love   \n",
       "\n",
       "                                                X-cc X-bcc  \\\n",
       "0  Meyers, Albert </O=ENRON/OU=NA/CN=RECIPIENTS/C...   NaN   \n",
       "1                                                NaN   NaN   \n",
       "2                                                NaN   NaN   \n",
       "3                                                NaN   NaN   \n",
       "4                                                NaN   NaN   \n",
       "\n",
       "                                            X-Folder    X-Origin  \\\n",
       "0             \\ExMerge - Solberg, Geir\\Deleted Items   SOLBERG-G   \n",
       "1            \\KPRESTO (Non-Privileged)\\Deleted Items    Presto-K   \n",
       "2   \\HARORA (Non-Privileged)\\Arora, Harry\\Sent Items     Arora-H   \n",
       "3  \\Vincent_Kaminski_Jun2001_1\\Notes Folders\\All ...  Kaminski-V   \n",
       "4  \\Darron_Giron_Jun2001\\Notes Folders\\All documents     Giron-D   \n",
       "\n",
       "                     X-FileName  \\\n",
       "0      geir solberg 6-26-02.PST   \n",
       "1  KPRESTO (Non-Privileged).pst   \n",
       "2   HARORA (Non-Privileged).pst   \n",
       "3                   vkamins.nsf   \n",
       "4                    dgiron.nsf   \n",
       "\n",
       "                                             content        user  \n",
       "0  \\r\\r\\n\\r\\r\\nStart Date: 1/30/02; HourAhead hou...   solberg-g  \n",
       "1  ----------------------------------------------...    presto-k  \n",
       "2   \\r\\r\\n-----Original Message-----\\r\\r\\nFrom: B...     arora-h  \n",
       "3  NOTE FROM MARK FREVERT\\r\\r\\n\\r\\r\\nWith the wid...  kaminski-v  \n",
       "4  ---------------------- Forwarded by Darron C G...     giron-d  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../enron/unlabeled/train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal, Lemmatization, and Stemming\n",
    "Stop words are common words, such as \"the\" or \"a\", that do not contribute to the overall meaning of our text.\n",
    "\n",
    "Lemmatization and stemming are special cases of normalization. They use different rulesets to reduce a word to its base form. Lemmatization uses dictionary lookup while stemming attempts to remove the end of a word (such as \"ing\" or \"ies\"), sometimes producing segments of words that are not linguistically correct. Lemmatization can also, theoretically, choose a base form from the context of word. For example, \"meeting\" can either be a noun or a verb and therefore requires context to determine its meaning. \n",
    "\n",
    "Our initial approach will use lemmatization over stemming, due to the tendency for stemming to leave word stems rather than valid words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important! To use spaCy, run in terminal/prompt:__\n",
    "\n",
    "```\n",
    "pip install spacy\n",
    "python -m spacy download en\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the clean function is to minimize different forms of words within the text. \n",
    "\n",
    "First, the function needs to convert all of the text to lowercase characters. The models we will use later would see \"Banana\" and \"banana\" as two different words due to the difference in capitalization. We do not want this kind of case-sensitivity and therefore must remove it by changing all letters to lowercase.\n",
    "\n",
    "The function must also remove forwarded text within a message. We want to avoid any message showing up in our dataset more than once; if we were to leave in text that was forwarded to multiple people, then that text would have more weight in our models.\n",
    "\n",
    "We will then lemmatize the text before applying stopword removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First, we want to add some common punctuation to the list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding some common punctuation to the spaCy stopwords\n",
    "for word in [\".\", \",\", \"!\", \"?\", \"\\\\r\\\\n\", \"\\\\r\\\\r\\\\n\", \"-PRON-\"]:\n",
    "    spacy.lang.en.stop_words.STOP_WORDS.add(word)\n",
    "    nlp.vocab[word].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(email_body):\n",
    "    # This converts all characters in the email to lowercase.\n",
    "    email_lower = \" \".join([i for i in email_body.lower().split()])\n",
    "    \n",
    "    # This line removes all forwarded text from an email.\n",
    "    no_forwards = email_lower.split('>from',1)[0].split('---',1)[0]\n",
    "    \n",
    "    # Lemmatization and stopword removal\n",
    "    # initialize spacy 'en' model\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    # Convert the sentence to a spacy document\n",
    "    document = nlp(email_body)\n",
    "    # Extract the lemma for each token\n",
    "    # If the lemma is not a stop word, then add it to the message using join()\n",
    "    # If the lemma is a stop word, then it is simply left out of the cleaned message\n",
    "    cleaned_message = \" \".join([token.lemma_ for token in document if token.lemma_ not in STOP_WORDS])\n",
    "\n",
    "    return cleaned_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function. If we pick an email at random, say number 13, this is the original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you're a doll.  thanks. on this one, feel free to control!  even if Cameron \n",
      "doesn't go up, prentice and i likely will, which means that we'll be stopping \n",
      "by napa (if you guys don't mind).\n"
     ]
    }
   ],
   "source": [
    "print(train['content'][13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is what the same email looks like after having been cleaned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doll   thank feel free control   cameron \n",
      " prentice likely mean stop \n",
      " napa ( guy mind )\n"
     ]
    }
   ],
   "source": [
    "result = clean(train['content'][13])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever spaCy encounters a word it thinks is a pronoun, it replaces it with -PRON-. Because we added this to the list of stopwords, we will not see this token in our cleaned messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean all the messages in our dataset. Then save them locally for faster loading in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
